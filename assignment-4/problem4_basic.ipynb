{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAT405 Assignment 4\n",
    "\n",
    "Noah Lanai - 9808252192 - 12h work\n",
    "\n",
    "Carl Hjalmarsson - 9305198930 - 12h work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdime\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import libraries for Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem **1 a)** and **b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the email content, decode them, and convert as dataframe\n",
    "def extract_files(files, m_class):  \n",
    "    rows = []\n",
    "    for fname in files:\n",
    "        # open the tar file\n",
    "        tfile = tarfile.open(fname, 'r:bz2')\n",
    "        for member in tfile.getmembers():\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                #get all the content of file as a row\n",
    "                rows.append({'message': row.decode('latin-1'), 'class': m_class})\n",
    "        tfile.close()\n",
    "    #return rows\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "#get one dataframe with all of our files\n",
    "ham_df = extract_files(['./20021010_easy_ham.tar.bz2','./20021010_hard_ham.tar.bz2'], 'ham')\n",
    "spam_df = extract_files(['./20021010_spam.tar.bz2'], 'spam')\n",
    "\n",
    "hamtrain, hamtest = train_test_split(ham_df, test_size = 0.25, random_state = 0)\n",
    "spamtrain, spamtest = train_test_split(spam_df, test_size = 0.25, random_state = 0)\n",
    "train_data = pd.concat([hamtrain, spamtrain])\n",
    "test_data = pd.concat([hamtest, spamtest])\n",
    "\n",
    "to_replace = {'ham': 0, 'spam': 1}\n",
    "train_data.replace(to_replace, inplace=True)\n",
    "test_data.replace(to_replace, inplace=True)\n",
    "\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem **2 a)** and **b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(binarize=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit nultinomial and Bernoulli naive Bayes classifiers\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['message']).toarray()\n",
    "y_train = train_data['class']\n",
    "X_test = vectorizer.transform(test_data['message']).toarray()\n",
    "y_test = test_data['class']\n",
    "\n",
    "# Laplace smoothing, alpha=1\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# binarize with value of 1\n",
    "bernb = BernoulliNB(binarize=0)\n",
    "bernb.fit(X_train, y_train)\n",
    "\n",
    "# Run multinomial and Bernoulli naive Bayes classifiers\n",
    "mnb_predict = mnb.predict(X_test)\n",
    "print(\"Accuracy score for the multinomial naive Bayes classifier:\\n\", metrics.accuracy_score(y_test, mnb_predict))\n",
    "\n",
    "bernb_predict = bernb.predict(X_test)\n",
    "print(\"Accuracy scorefor the Bernoulli naive Bayes classifier:\\n\", metrics.accuracy_score(y_test, bernb_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Differences between models**\n",
    "The multinomial naive Bayes model implements the naive Bayes algorithm for multinomially distributed data, which is a generalisation of the binomial distribution. For n number of trials it calculates the probability of **x_k** number of successes for **k** different features, i.e. the probability of finding **k** different words in the same text **x_k** number of times. The algorithm calculates the maximum likelihood of a specific word occuring given a specified class, to be used in the naive Bayes formula, based on word count vectors. If a word does not occur it is ignored in the calculation of the maximum likelihood.\n",
    "\n",
    "The Bernoulli naive Bayes model implements the naive Bayes algorithm for multivariate Bernoulli distributed data, which means that each feature has to be condensed to a binary value, which is done using the binarize parameter to provide a threshold for feature count. This gives binary-valued feature vectors, meaning that the word counts for each individual word is condensed to either 1 or 0 depending on the threshold. The algorithm calculates the likelihood of a specific word occuring, to be used in the naive Bayes formula, based on these binary-valued feature vectors. This version of calculating the maximum likelihood also penalizes non-occurrence of a word. \n",
    "\n",
    "The main differences are the distributions used to calculate the probability terms of the naive Bayes formula. Adding to this, the main difference seems to be in how it calculates the likelihood terms, and what inputs to use for this calculation; the feature vectors.\n",
    "\n",
    "#### **Choice of binarize**\n",
    "The Bernoulli model seemed to fair worse when increasing the binarize variable from the default value of 0, which is why we decided to keep the default value.\n",
    "\n",
    "#### Differences in performance\n",
    "Referring to the scikit-learn.org page, it states that the Bernoulli naive Bayes model might perform better on some datasets, especially those with shorter documents. We found this not to be the case since email messages are usually quite short in comparison to other texts. When discarding the word counts we lose information which might impact the result,\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8230433a90deedfd204f402afa77435b9f1612df21a8fe919bc9fb2fb8e7ebcb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('dat405': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
