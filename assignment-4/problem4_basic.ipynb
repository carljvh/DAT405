{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAT405 Assignment 4\n",
    "\n",
    "Noah Lanai - 9808252192 - 12h work\n",
    "\n",
    "Carl Hjalmarsson - 9305198930 - 12h work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import nbdime\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem **1 a)** and **b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract files and convert to a dataframe. Label \"ham\" and \"spam\" for readability\n",
    "def extract_files(files, m_class):  \n",
    "    rows = []\n",
    "    for fname in files:\n",
    "        tfile = tarfile.open(fname, 'r:bz2')\n",
    "        for member in tfile.getmembers():\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                rows.append({'message': row.decode('latin-1'), 'class': m_class})\n",
    "        tfile.close()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Get data for different class of email\n",
    "ham_df = extract_files(['./20021010_easy_ham.tar.bz2','./20021010_hard_ham.tar.bz2'], 'ham')\n",
    "spam_df = extract_files(['./20021010_spam.tar.bz2'], 'spam')\n",
    "\n",
    "# Get data for each type of ham\n",
    "eazy_ham_df = extract_files(['./20021010_easy_ham.tar.bz2'], 'ham')\n",
    "hard_ham_df = extract_files(['./20021010_hard_ham.tar.bz2'], 'ham')\n",
    "\n",
    "# Split up data into train and test\n",
    "hamtrain, hamtest = train_test_split(ham_df, test_size = 0.25, random_state = 0)\n",
    "spamtrain, spamtest = train_test_split(spam_df, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Merge data to one big set\n",
    "train_data = pd.concat([hamtrain, spamtrain])\n",
    "test_data = pd.concat([hamtest, spamtest])\n",
    "\n",
    "# Add spam to ham\n",
    "eazy_hamspam_df = pd.concat([eazy_ham_df, spam_df])\n",
    "hard_hamspam_df = pd.concat([hard_ham_df, spamtrain])\n",
    "\n",
    "# Replacing classes with binary numbers\n",
    "to_replace = {'ham': 0, 'spam': 1}\n",
    "train_data.replace(to_replace, inplace=True)\n",
    "test_data.replace(to_replace, inplace=True)\n",
    "eazy_hamspam_df.replace(to_replace, inplace=True)\n",
    "hard_hamspam_df.replace(to_replace, inplace=True)\n",
    "\n",
    "# Resetting index\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "eazy_hamspam_df.reset_index(drop=True, inplace=True)\n",
    "hard_hamspam_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem **2 a)** and **b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the multinomial naive Bayes classifier:\n",
      " 0.9794437726723095\n",
      "Accuracy scorefor the Bernoulli naive Bayes classifier:\n",
      " 0.8621523579201935\n"
     ]
    }
   ],
   "source": [
    "# Fit nultinomial and Bernoulli naive Bayes classifiers\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['message']).toarray()\n",
    "y_train = train_data['class']\n",
    "X_test = vectorizer.transform(test_data['message']).toarray()\n",
    "y_test = test_data['class']\n",
    "\n",
    "# Laplace smoothing, alpha=1\n",
    "mnb = MultinomialNB(alpha = 1)\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# binarize with value of 1\n",
    "bernb = BernoulliNB(binarize = 1)\n",
    "bernb.fit(X_train, y_train)\n",
    "\n",
    "# Run multinomial and Bernoulli naive Bayes classifiers\n",
    "mnb_predict = mnb.predict(X_test)\n",
    "print(\"Accuracy score for the multinomial naive Bayes classifier:\\n\", metrics.accuracy_score(y_test, mnb_predict))\n",
    "\n",
    "bernb_predict = bernb.predict(X_test)\n",
    "print(\"Accuracy scorefor the Bernoulli naive Bayes classifier:\\n\", metrics.accuracy_score(y_test, bernb_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Differences between models**\n",
    "The multinomial naive Bayes model implements the naive Bayes algorithm for multinomially distributed data, which is a generalisation of the binomial distribution. For n number of trials it calculates the probability of **x_k** number of successes for **k** different features, i.e. the probability of finding **k** different words in the same text **x_k** number of times. The algorithm calculates the maximum likelihood of a specific word occuring given a specified class, to be used in the naive Bayes formula, based on word count vectors. If a word does not occur it is ignored in the calculation of the maximum likelihood.\n",
    "\n",
    "The Bernoulli naive Bayes model implements the naive Bayes algorithm for multivariate Bernoulli distributed data, which means that each feature has to be condensed to a binary value, which is done using the binarize parameter to provide a threshold for feature count. This gives binary-valued feature vectors, meaning that the word counts for each individual word is condensed to either 1 or 0 depending on the threshold. The algorithm calculates the likelihood of a specific word occuring, to be used in the naive Bayes formula, based on these binary-valued feature vectors. This version of calculating the maximum likelihood also penalizes non-occurrence of a word.\n",
    "\n",
    "The main differences are the distributions used to calculate the probability terms of the naive Bayes formula. Adding to this, the main difference seems to be in how it calculates the likelihood terms, and what inputs to use for this calculation; the feature vectors.\n",
    "\n",
    "#### **Choice of binarize**\n",
    "The Bernoulli model seemed to fair worse when increasing the binarize variable from the default value of 0, which is why we decided to keep the default value.\n",
    "\n",
    "#### Differences in performance\n",
    "Referring to the scikit-learn.org page, it states that the Bernoulli naive Bayes model might perform better on some datasets, especially those with shorter documents. We found this not to be the case when comparing since email messages are usually quite short in comparison to other texts. When discarding the word counts we lose information. If we for example set the binary parameter to two, then every word that occurs once will be set to zero. These words will not be included in the classification which might impact the result. For example if the word \"invest\" occurs one time in an email. This will not necessarily increase the probability of spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 **i. and ii.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eazy ham vs spam: Accuracy score for the multinomial naive Bayes classifier:\n",
      " 0.9941022280471822\n",
      "Eazy ham vs spam: Accuracy scorefor the Bernoulli naive Bayes classifier:\n",
      " 0.858781127129751\n",
      "Hard ham vs spam: Accuracy score for the multinomial naive Bayes classifier:\n",
      " 0.9792\n",
      "Hard ham vs spam: Accuracy scorefor the Bernoulli naive Bayes classifier:\n",
      " 0.4624\n"
     ]
    }
   ],
   "source": [
    "# Eazy ham vs spam classifier\n",
    "X_eazy = vectorizer.transform(eazy_hamspam_df['message']).toarray()\n",
    "y_eazy = eazy_hamspam_df['class']\n",
    "\n",
    "mnb_predict = mnb.predict(X_eazy)\n",
    "print(\"Eazy ham vs spam: Accuracy score for the multinomial naive Bayes classifier:\\n\", metrics.accuracy_score(y_eazy, mnb_predict))\n",
    "\n",
    "bernb_predict = bernb.predict(X_eazy)\n",
    "print(\"Eazy ham vs spam: Accuracy scorefor the Bernoulli naive Bayes classifier:\\n\", metrics.accuracy_score(y_eazy, bernb_predict))\n",
    "\n",
    "# Hard ham vs spam classifier\n",
    "X_hard = vectorizer.transform(hard_hamspam_df['message']).toarray()\n",
    "y_hard = hard_hamspam_df['class']\n",
    "\n",
    "mnb_predict = mnb.predict(X_hard)\n",
    "print(\"Hard ham vs spam: Accuracy score for the multinomial naive Bayes classifier:\\n\", metrics.accuracy_score(y_hard, mnb_predict))\n",
    "\n",
    "bernb_predict = bernb.predict(X_hard)\n",
    "print(\"Hard ham vs spam: Accuracy scorefor the Bernoulli naive Bayes classifier:\\n\", metrics.accuracy_score(y_hard, bernb_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences in performance\n",
    "The best accuracy score was for the naive Bayes classifier for eazy ham. The data set was predicted with over 99% accuracy. Eazy ham had the best score for both the methods. The lowest accuracy score where the Bernoulli naive Bayes classifier for the hard ham data set. The conclution is that the best method to use is the multinomial naive Bayes classifier, since it performs best for both data sets, and that eazy ham is easier for the computer to predict."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dede480c75803c6afe94270c90f6b71384aaa9607279584a745202969843126"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_ai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
